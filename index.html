<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MAIR</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" type="image/png" href="./index_files/icon.ico">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>MAIR:</b> Multi-view Attention Inverse Rendering <br> with 3D Spatially-Varying Lighting Estimation
            <br /><br />
            <small>
                CVPR 2023
            </small>
            <br /><br />
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://bring728.github.io/junyong-website/">
                          JunYong Choi
                        </a><sup>1,2</sup>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/seokyeong-lee-852a96191/?originalSubdomain=kr">
                        SeokYeong Lee
                        </a><sup>1,2</sup>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/haesol-park-06a7b216b/?originalSubdomain=kr">
                        Haesol Park
                        </a><sup>1</sup>
                    </li>
                     <li>
                        <a href="https://sites.google.com/view/deepiplab/">
                          Seung-Won Jung
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/ijkim/%ED%99%88">
                          Ig-Jae Kim
                        </a><sup>1,3,4</sup>
                    </li>
                    <li>
                        <a href="https://junghyuncho.notion.site/Junghyun-Cho-36f85eff362148dab9e23e6628fe3551">
                        Junghyun Cho
                        </a><sup>1,3,4</sup>
                    </li>
                </ul>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>Korea Institute of Science and Technology(KIST) 
                    </li>
                    <li>
                        <sup>2</sup>Korea University
                    </li>
                    <br>
                    <li>
                        <sup>3</sup>AI-Robotics, KIST School, University of Science and Technology
                    </li>
                    <br>
                    <li>
                        <sup>4</sup>Yonsei-KIST Convergence Research Institute, Yonsei University
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2303.12368">
                            <img src="./index_files/mair_paper_icon.png" height="120px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!-- <a href="http://cseweb.ucsd.edu/~viscomp/projects/CVPR21OpenRooms/video.mp4"> -->
                            <img src="./index_files/youtube_icon_dark.png" height="120px"><br>
                                <h4><strong>Technical Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/bring728/OpenRooms_FF">
                            <img src="./index_files/dataNew.png" height="120px"><br>
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    We propose a scene-level inverse rendering framework that uses multi-view images to decompose the scene into geometry, a SVBRDF, and 3D spatially-varying lighting. Because multi-view images provide a variety of information about the scene, multi-view images in object-level inverse rendering have been taken for granted. However, owing to the absence of multi-view HDR synthetic dataset, scene-level inverse rendering has mainly been studied using single-view image. We were able to successfully perform scene-level inverse rendering using multi-view images by expanding OpenRooms dataset and designing efficient pipelines to handle multi-view images, and splitting spatially-varying lighting. Our experiments show that the proposed method not only achieves better performance than single-view-based methods, but also achieves robust performance on unseen real-world scene. Also, our sophisticated 3D spatially-varying lighting volume allows for photorealistic object insertion in any 3D location.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Pipeline
                </h3>
                <img src="./index_files/mair_framework.png" class="img-responsive" alt="overview"><br>
                
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    OpenRooms FF, Multi-view extension of <a href="https://vilab-ucsd.github.io/ucsd-openrooms/">OpenRooms</a>
                </h3>
                <img src="./index_files/dataset_teaser.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>


        <div class="row">
            <div class="col-md-12">
                <h3>
                    Comparisons
                </h3>
                <img src="./index_files/ir_result.png" class="img-responsive" alt="overview"><br>
                <img src="./index_files/oi_1_img.png" class="img-responsive" alt="overview"><br>
                <img src="./index_files/oi_2_img.png" class="img-responsive" alt="overview"><br>
                
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Object Insertion
                </h3>
                <div class="text-center">
                    <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                        <source src="index_files/oi1_html.mp4" type="video/mp4">
                    </video>
                    <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                        <source src="index_files/oi2_html.mp4" type="video/mp4">
                    </video>
                    <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                        <source src="index_files/oi3_html.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    View Synthesis + Chrome Sphere Insertion
                </h3>
                <div class="text-center">
                    <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                        <source src="index_files/viewsyn1_html.mp4" type="video/mp4">
                    </video>
                    <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                        <source src="index_files/viewsyn2_html.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT)(No.2020-0-00457, 50%) and KIST Institutional Program(Project No.2E32301, 50%). <br>
                Images used for object insertion were imported from <a href="https://github.com/googleinterns/IBRNet">IBRNet</a>. The view synthesis results were created using <a href="https://github.com/sunset1995/DirectVoxGO">DirectVoxGo</a>. The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://vilab-ucsd.github.io/ucsd-openrooms/">OpenRooms (Zhengqin Li)</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
